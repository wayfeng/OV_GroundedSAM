{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e75afa5-4dec-4c4b-97ae-e60eb97441f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File existed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#DATA_URL = \"https://ultralytics.com/assets/coco128.zip\"\n",
    "DATA_DIR= Path('./data')\n",
    "ZIP_DIR = DATA_DIR/'coco128.zip'\n",
    "\n",
    "if not (DATA_DIR / \"coco128/images/train2017\").exists():\n",
    "    with ZipFile(ZIP_DIR, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "else:\n",
    "    print(\"File existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f9f5e6-8601-475b-a04d-0c3fc15e649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class COCOLoader(data.Dataset):\n",
    "    def __init__(self, images_path):\n",
    "        self.images = list(Path(images_path).iterdir())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = cv2.imread(str(image_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "coco_dataset = COCOLoader(DATA_DIR / 'coco128/images/train2017')\n",
    "calibration_loader = torch.utils.data.DataLoader(coco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e459486-67e5-4476-aeb2-8d0533363a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "import nncf\n",
    "\n",
    "def transform_fn(image_data):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
    "    Parameters:\n",
    "        image_data: image data produced by DataLoader during iteration\n",
    "    Returns:\n",
    "        input_tensor: input data in Dict format for model quantization\n",
    "    \"\"\"\n",
    "    image = image_data.numpy()\n",
    "    processed_image = preprocess_image(np.squeeze(image))\n",
    "    return processed_image\n",
    "\n",
    "calibration_dataset = nncf.Dataset(calibration_loader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396432a4-6511-41ac-9ad8-6882e76a13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "from torchvision.transforms.functional import resize, to_pil_image \n",
    "import numpy as np\n",
    "\n",
    "class ResizeLongestSide:\n",
    "    \"\"\"\n",
    "    Resizes images to longest side 'target_length', as well as provides\n",
    "    methods for resizing coordinates and boxes. Provides methods for\n",
    "    transforming numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_length: int) -> None:\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def apply_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        target_size = self.get_preprocess_shape(image.shape[0], image.shape[1], self.target_length)\n",
    "        return np.array(resize(to_pil_image(image), target_size))\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(\n",
    "            original_size[0], original_size[1], self.target_length\n",
    "        )\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array shape Bx4. Requires the original image size\n",
    "        in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Compute the output size given input size and target long side length.\n",
    "        \"\"\"\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww = int(neww + 0.5)\n",
    "        newh = int(newh + 0.5)\n",
    "        return (newh, neww)\n",
    "\n",
    "\n",
    "resizer = ResizeLongestSide(1024)\n",
    "\n",
    "\n",
    "def preprocess_image(image: np.ndarray):\n",
    "    resized_image = resizer.apply_image(image)\n",
    "    resized_image = (resized_image.astype(np.float32) - [123.675, 116.28, 103.53]) / [58.395, 57.12, 57.375]\n",
    "    resized_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)).astype(np.float32), 0)\n",
    "\n",
    "    # Pad\n",
    "    h, w = resized_image.shape[-2:]\n",
    "    padh = 1024 - h\n",
    "    padw = 1024 - w\n",
    "    x = np.pad(resized_image, ((0, 0), (0, 0), (0, padh), (0, padw)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa1bf1c-b47d-4533-a1fb-ab760316d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369102ccc12e4f04acb96bea52105d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f71150203df435086e795b3d57325c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:96 ignored nodes were found by name in the NNCFGraph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacf0ce8aa3942bfbea18188d0afe720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2389a58d554990a9795a0b1cd63450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model quantization finished\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "core = ov.Core()\n",
    "\n",
    "ov_encoder_path = Path('./models/sam/sam_image_encoder_vit_h.xml')\n",
    "model = core.read_model(ov_encoder_path)\n",
    "quantized_model = nncf.quantize(model,\n",
    "                                calibration_dataset,\n",
    "                                model_type=nncf.parameters.ModelType.TRANSFORMER,\n",
    "                                preset=nncf.common.quantization.structs.QuantizationPreset.MIXED, subset_size=128)\n",
    "print(\"model quantization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18ab581-3a35-43e0-9f5c-16e2cd931ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.save_model(quantized_model, 'models/sam/sam_image_encoder_vit_h_quant.xml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
